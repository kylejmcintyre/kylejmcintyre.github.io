<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Kyle McIntyre - Software Engineering</title><link href="https://kylemcintyre.com/" rel="alternate"></link><link href="https://kylemcintyre.com/feeds/software-engineering.atom.xml" rel="self"></link><id>https://kylemcintyre.com/</id><updated>2021-02-23T20:34:00-07:00</updated><subtitle>Software engineering, data science, hobby farming and other pursuits</subtitle><entry><title>Clean install of JuptyerHub and JupyterLab on Ubuntu w/ Google OAuth &amp; qgrid</title><link href="https://kylemcintyre.com/clean-install-of-juptyerhub-and-jupyterlab-on-ubuntu-w-google-oauth-qgrid.html" rel="alternate"></link><published>2021-02-23T20:34:00-07:00</published><updated>2021-02-23T20:34:00-07:00</updated><author><name>Kyle McIntyre</name></author><id>tag:kylemcintyre.com,2021-02-23:/clean-install-of-juptyerhub-and-jupyterlab-on-ubuntu-w-google-oauth-qgrid.html</id><summary type="html">&lt;p&gt;I recently undertook the project of installing JupyterHub and JupyterLab on our 
development servers at Quiq. The servers are currently running Ubuntu 18.04.+ and are managed via Ansible. Users
access the servers solely via SSH and leverage port forwarding. My goal was to give each
user with an existing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently undertook the project of installing JupyterHub and JupyterLab on our 
development servers at Quiq. The servers are currently running Ubuntu 18.04.+ and are managed via Ansible. Users
access the servers solely via SSH and leverage port forwarding. My goal was to give each
user with an existing account access to their own JupyterLab workspace (via the hub), with access
to a shared directory for file/notebook exchange. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://tljh.jupyter.org/en/latest/"&gt;TLJH&lt;/a&gt; is a popular option, but I wasn't too keen on some of the more
opinionated choices it made, or perhaps I just wasn't comfortable with not understanding its inner-workings. So,
I endeavored to install the projects directly. I used &lt;a href="https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html"&gt;this guide from JupyterHub&lt;/a&gt; 
as a reference for how to install them 'from the ground up'. It was an immensely helpful resource, but it has some quirks/drawbacks that I 
tried to address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It isn't in a reproducible/repeatable form&lt;/li&gt;
&lt;li&gt;It uses a virtual env based on the system python for the JuptyerHub runtime, and a separate conda-based runtime for the user kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second point was particularly problematic. For one, basing anything off the system python gives you older versions of everything and makes it tempting 
to mess with the system python to resolve dependency hell issues - which is obviously a bad idea. The other problem
was caused by version mismatches between the jupyter hub runtime and the kernel runtime. Specifically, I struggled to get the awesome qgrid project
to work with symptoms identical to &lt;a href="https://github.com/quantopian/qgrid/issues/183"&gt;this issue&lt;/a&gt; which sites version mismatches as the likely culprit.
I eventually scrapped the approach of having two separate python runtimes and just focused on a single conda-based runtime. Having two runtimes
would've been fine too - the main problems came from basing anything off of the system python, but I collapsed down to just a single runtime for simplicity. &lt;/p&gt;
&lt;h3&gt;The Goods&lt;/h3&gt;
&lt;p&gt;The non-ansibled result of my efforts are in available in &lt;a href="https://gist.github.com/kylejmcintyre/3c0bd9a70c8693d50d23203bbadbce5d"&gt;this gist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following are notable things about this gist relative to Jupyter's 'from the ground up' guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It assumes miniconda has been installed to /opt/miniconda&lt;/li&gt;
&lt;li&gt;It creates and uses a single new conda-based python environment that is completely separate from the system python&lt;/li&gt;
&lt;li&gt;It doesn't install a nodejs-based reverse proxy server - users access JupyterHub by establishing port forwarding on port 8889&lt;/li&gt;
&lt;li&gt;It installs the OAuthenticator project and leverages Google OAuth to sign users in&lt;/li&gt;
&lt;li&gt;The Google OAuth callback assumes users forward the port locally through 8089&lt;/li&gt;
&lt;li&gt;It installs the qgrid package &amp;amp; extension&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes, it's a bit redundant that my users must go through the Google OAuth flow since they must be logged into the server 
in order to forward the necessary port. But JuptyerHub has no way to know that and the users breeze through it. I confirmed 
that when using Google OAuth, if the username preceding the @ in the Google account matches a user account on the server, 
that account is used for starting the kernel and their home directory is visible in the file explorer. If your users' Google
accounts don't match their usernames on your servers, I believe you can fix this via &lt;a href="https://jupyterhub.readthedocs.io/en/stable/reference/authenticators.html"&gt;username normalization&lt;/a&gt; - but I didn't have to.&lt;/p&gt;
&lt;p&gt;In order to accomplish basic file sharing, I simply created a shared directory on the servers and insured that all users had
a &lt;code&gt;shared&lt;/code&gt; symlink in their home directory pointing to that location. In the end, the install script and setup is fairly minimal
but it took me enough trial and error that I wanted to share it with the world. Cheers!&lt;/p&gt;
&lt;script src="https://gist.github.com/kylejmcintyre/3c0bd9a70c8693d50d23203bbadbce5d.js"&gt;&lt;/script&gt;</content><category term="Software Engineering"></category><category term="python"></category><category term="jupyter"></category><category term="ubuntu"></category><category term="qgrid"></category></entry><entry><title>Using Athena to convert individual JSON objects to Avro files</title><link href="https://kylemcintyre.com/using-athena-to-convert-individual-json-objects-to-avro-files.html" rel="alternate"></link><published>2021-02-12T10:20:00-07:00</published><updated>2021-02-12T10:20:00-07:00</updated><author><name>Kyle McIntyre</name></author><id>tag:kylemcintyre.com,2021-02-12:/using-athena-to-convert-individual-json-objects-to-avro-files.html</id><summary type="html">&lt;p&gt;Back in September of 2019, AWS added support to Athena to enable Create Table as Select (CTAS) statements,
as well as INSERT INTO statements. These statements allow the the results of a query to be used
to construct a new table (CTAS) or appended to another table (INSERT INTO). This …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back in September of 2019, AWS added support to Athena to enable Create Table as Select (CTAS) statements,
as well as INSERT INTO statements. These statements allow the the results of a query to be used
to construct a new table (CTAS) or appended to another table (INSERT INTO). This was a big step forward for
Athena because having this support enables it to fulfill some ETL needs. A primary use case is to convert the format
of data that underlies an Athena table.&lt;/p&gt;
&lt;p&gt;It's certainly not unusual for apps to produce individual JSON records and store them as objects in S3. And
with Athena, you can define a lazy schema that enables Presto (under the hood of Athena) to do some nice
distributed queries against them (asynchronously). However, a bunch of individual objects result in poorer
query performance as a dataset gets larger. So it's recommended to convert your data into a 'big data' file
format such as ORC, Parquet or Avro. Each of these formats have tradeoffs, but Avro is definitely the
outlier of the three under discussion. ORC and Parquet are columnar formats, whereas Avro is row-based. That
makes the structure of Avro more akin to a relational database table or spreadsheet, albeit very nicely 
compressed and with its schema declared in the file header. Roughly speaking, the columnar formats are what
you want to use if you're planning to do lots of 'analytical' queries involving a few columns, such as sums 
and averages grouped by certain columns. However, if you're planning to essentially select a bunch of full rows
back out of your big data files, Avro is arguably a better fit since reconstructing full rows out of columnar
formats can involve a lot of jumping around, especially if your data is rather wide. In a recent use case I needed
to support fetching a lot of full rows of fairly wide data, so I endeavored to get Athena to do a conversion from JSON to Avro for me. 
In the end it all worked great and I very much enjoyed the serverless and mostly "codeless" approach. However,
I did hit one snag worth sharing with the world.&lt;/p&gt;
&lt;p&gt;To set the stage, consider the following Athena table declarations, which are actually Hive DDL:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;EXTERNAL&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="k"&gt;IF&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;EXISTS&lt;/span&gt; &lt;span class="n"&gt;activity_json_staging&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;activityId&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;startTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;endTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;contactState&lt;/span&gt; &lt;span class="n"&gt;STRUCT&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;creationDate&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="k"&gt;ROW&lt;/span&gt; &lt;span class="n"&gt;FORMAT&lt;/span&gt; &lt;span class="n"&gt;serde&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;org.apache.hive.hcatalog.data.JsonSerDe&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;LOCATION&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;s3://foo-bucket/activity-json-staging&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;EXTERNAL&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="k"&gt;IF&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;EXISTS&lt;/span&gt; &lt;span class="n"&gt;activity_avro_base&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;activityId&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;startTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;endTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;contactState&lt;/span&gt; &lt;span class="n"&gt;STRUCT&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;PARTITIONED&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;creationDate&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="k"&gt;ROW&lt;/span&gt; &lt;span class="n"&gt;FORMAT&lt;/span&gt; &lt;span class="n"&gt;serde&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;org.apache.hadoop.hive.serde2.avro.AvroSerDe&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;WITH&lt;/span&gt; &lt;span class="n"&gt;SERDEPROPERTIES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avro.schema.literal&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;type&amp;quot; : &amp;quot;record&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;name&amp;quot; : &amp;quot;ContactActivityRecord&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;namespace&amp;quot; : &amp;quot;default&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;fields&amp;quot; : [ {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;activityId&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;string&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;  }, {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;startTime&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;long&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;  }, {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;endTime&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;long&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;  }, {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;contactState&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : {&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;type&amp;quot; : &amp;quot;record&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;name&amp;quot; : &amp;quot;Contact&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;namespace&amp;quot; : &amp;quot;default&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;fields&amp;quot; : [ {&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;quot;name&amp;quot; : &amp;quot;raw&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;string&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;      } ]&lt;/span&gt;
&lt;span class="s1"&gt;    }&lt;/span&gt;
&lt;span class="s1"&gt;  } ]&lt;/span&gt;
&lt;span class="s1"&gt;}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;STORED&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;AVRO&lt;/span&gt;
&lt;span class="k"&gt;LOCATION&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;s3://foo-bucket/activity-avro-base&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first table is a JSON-backed table, while the second is Avro-backed. This is determined by the serde declaration. With Avro, you're expected to provide
an Avro schema declaration and pass it in as a literal via SERDEPROPERTIES. This is kind of annoying since it feels like you're just repeating the Hive schema in another dialect. 
An important exception to that statement is that you don't declare avro schema for any fields that are part of the table's partition declaration. 
In the end I created a POJO-like data structure for declaring what an Athena table looks like, and generated the Hive DDL and Avro schema from that. &lt;/p&gt;
&lt;p&gt;So the general idea is to use an INSERT INTO statement to take the individual JSON records underlying the activity_json_staging table and move them 
into activity_avro_base. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;activity_json_staging&lt;/span&gt; &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;activity_avro_base&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As a byproduct of them being moved, their format will be changed, they'll be batched and they'll be partitioned because
the second table has a partition declared. Pretty slick, right?  However, upon testing the data transfer, I was confronted with a 
runtime error that provided essentially zero guidance and for which I couldn't find any relevant resources on the internet!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;HIVE_WRITER_DATA_ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;HIVE_WRITER_DATA_ERROR&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;If&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="n"&gt;manifest&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;generated&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; 
&lt;span class="s1"&gt;&amp;#39;s3://foo-bucket/query-results/e52567d6-b56f-4fde-8514-8887d6f58f22-manifest.csv&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; 
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;may&lt;/span&gt; &lt;span class="n"&gt;need&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;manually&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;locations&lt;/span&gt; &lt;span class="n"&gt;specified&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; 
&lt;span class="n"&gt;Athena&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;delete&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;your&lt;/span&gt; &lt;span class="n"&gt;account&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And...that was it. I scoured the API results, CLI output and the AWS UI for more information but couldn't find any. And all Google turned up was the &lt;a href="https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/RecordFileWriter.java"&gt;Presto source code&lt;/a&gt; that presumably emits it. 
So, it was one of those memorable experiences where you basically binary search your schema/input for the problem until you pinpoint it. In the end,
the problem was that one of my JSON records had the value &lt;code&gt;null&lt;/code&gt; for contactState. The Hive DDL was fine with this, but the Avro schema was not. The fix was to 
change the Avro declaration for the contactState field as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;contactState&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;record&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Contact&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;namespace&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;raw&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;string&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;default&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;
      &lt;span class="p"&gt;}]&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that its type property is now an array and includes a string value of "null". Yep, that was it. Silly me! Things have been working great since then but I have lingering concerns about receiving such 
minimal diagnostic info on what seems like a pretty common mistake. Anyway, sharing it here in hopes it helps someone else. You may be wondering why I'm doing this in Athena instead of explicitly in Glue,
and I guess the answer is to limit my cloud-specific dependencies. &lt;/p&gt;</content><category term="Software Engineering"></category><category term="athena"></category><category term="avro"></category></entry></feed>