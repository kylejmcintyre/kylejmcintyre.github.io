<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Kyle McIntyre - Kyle McIntyre</title><link href="https://kylemcintyre.com/" rel="alternate"></link><link href="https://kylemcintyre.com/feeds/kyle-mcintyre.atom.xml" rel="self"></link><id>https://kylemcintyre.com/</id><updated>2021-04-08T20:17:00-06:00</updated><subtitle>Software engineering, data science, hobby farming and other pursuits</subtitle><entry><title>Roll the Dice: A fun probability experiment for all ages</title><link href="https://kylemcintyre.com/roll-the-dice-a-fun-probability-experiment-for-all-ages.html" rel="alternate"></link><published>2021-04-08T20:17:00-06:00</published><updated>2021-04-08T20:17:00-06:00</updated><author><name>Kyle McIntyre</name></author><id>tag:kylemcintyre.com,2021-04-08:/roll-the-dice-a-fun-probability-experiment-for-all-ages.html</id><summary type="html">&lt;p&gt;Last fall I started teaching coding concepts at the local grade school on a volunteer basis. Every month I spend
roughly 2 hours conducting a workshop for approximately 15 students. We've had a lot of fun and I've enjoyed the challenge
of coming up with meaningful lessons and projects. The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Last fall I started teaching coding concepts at the local grade school on a volunteer basis. Every month I spend
roughly 2 hours conducting a workshop for approximately 15 students. We've had a lot of fun and I've enjoyed the challenge
of coming up with meaningful lessons and projects. The biggest challenge has been that my students range from 3rd graders to 8th graders. 
This is rural Montana after all. So most of my effort is spent thinking of good workshops rather than coding prep.  &lt;/p&gt;
&lt;p&gt;To kick off each workshop, I like to pose a 'random question of the day' that sets the stage for the workshop. 
At our last workshop, the random question was as follows: &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What number are you most likely to get when you roll two dice and add them together? &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This question led me to put together this simple project that even my 1st grader seemed to grok.&lt;/p&gt;
&lt;h3&gt;Roll the Dice&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Prerequisites: the student must know their numbers and be able to perform addition of small numbers&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Estimated Time: 30 minutes&lt;/em&gt;&lt;/p&gt;
&lt;h5&gt;1. Find a pair of dice and a piece of paper and pencil (or a chalkboard, or a Google doc...)&lt;/h5&gt;
&lt;h5&gt;2. Ask your student(s) some leading questions&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;What are all of the possible outcomes of rolling two dice and adding them together? &lt;ul&gt;
&lt;li&gt;Young kids: "What's the biggest number we could get?" "What's the smallest number we could get?" "Can we get every number inbetween?"&lt;/li&gt;
&lt;li&gt;&lt;em&gt;(Answer: 2, 3 ... 11, 12)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Are all the outcomes equally likely? &lt;ul&gt;
&lt;li&gt;Young kids: "Will we get 2 as often as we get 9?" "Which number will we get the most or are they all the same?"&lt;/li&gt;
&lt;li&gt;&lt;em&gt;(Answer: no, but don't tell them yet)&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What are some ways we could figure out the answer to the last question? &lt;ul&gt;
&lt;li&gt;Some older wiz kids might be able to come up with the theoretical proof&lt;/li&gt;
&lt;li&gt;But we're generally trying to push them towards the idea that we could run an experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;3. A manual experiment&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Have the student(s) roll the dice and add them together at least 20 times. Keep tally marks of how many times they roll each possible result. You might get something like this:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;2  - |
3  - 
4  - ||
5  - ||
6  - ||||
7  - ||
8  - |||
9  - |||
10 - 
11 - |||
12 - 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Note: if you do this neatly with good spacing, you'll end up with a poor man's histogram&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ask the student which outcome seems to be most likely based on the experiment&lt;/li&gt;
&lt;li&gt;If one of the outcomes never happens, ask the student if they can assume it will never happen based on their experiment - they'll probably respond no!&lt;/li&gt;
&lt;li&gt;What's wrong with our experiment then? Why can't we trust its results? &lt;ul&gt;
&lt;li&gt;&lt;em&gt;Answer: we need a much larger sample set, i.e. we didn't roll the dice enough times to really get a good idea of what happens&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;4. A computer experiment&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;We'd like to roll the dice 10,000 times so that we can feel good about our experiment&lt;ul&gt;
&lt;li&gt;Ask the student if they're up for it 😂&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Instead of doing the physical experiment 10,000 times, we'll have the computer do it for us...&lt;/li&gt;
&lt;li&gt;Visit &lt;a href="https://replit.com/@avoncoders/RollTheDice" target=new&gt;https://replit.com/@avoncoders/RollTheDice&lt;/a&gt; to get my code for the experiment and click &lt;code&gt;Fork&lt;/code&gt; in the upper right&lt;/li&gt;
&lt;li&gt;You may need to create an account - you can do Google single sign-on&lt;/li&gt;
&lt;li&gt;Once you have your own copy of the code, click the green run button in the top middle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When you run the code, it will perform the same experiment with 20 rolls, and produce a dubious result like this, similar to the manual experiment:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/dice20.png" alt="dice20" width="300"/&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now, press the grey &lt;code&gt;Stop&lt;/code&gt; button in the middle top. Change the value of numberOfRolls to something like 100. Press &lt;code&gt;Run&lt;/code&gt; again&lt;/li&gt;
&lt;li&gt;Repeat the above step for 100, 1000 and 10000 and see how the results take shape&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src="images/dice100.png" alt="dice100" width="300"/&gt;
&lt;img src="images/dice1000.png" alt="dice1000" width="300"/&gt;
&lt;img src="images/dice10000.png" alt="dice10000" width="300"/&gt;&lt;/p&gt;
&lt;h5&gt;5. Conclusions&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Clearly, not all outcomes are equally likely&lt;/li&gt;
&lt;li&gt;Which number is the most likely? 7&lt;ul&gt;
&lt;li&gt;Why? Because there are many more ways to get a 7 then there are to get a 12. To roll a twelve, you need two sixes. To roll a seven you could have any combination of (1, 6), (2, 5), (3, 4)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If our experiment doesn't have enough trials/samples, we can't trust the result. If we ran the experiment infinite times, we'd have the exact probability/chances of each outcome&lt;/li&gt;
&lt;li&gt;If your student(s) have played Settlers of Catan, they'll now understand why you move the Robber whenever a 7 is rolled, and why the tiles associated with 6 and 8 are highly coveted&lt;/li&gt;
&lt;li&gt;The computer was able to run an experiment that would've taken us days in less than a second. That is amazing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After going through this with my first grader, I asked him "If someone approached you on the playground and offered to pay you a dollar for guessing the sum of two dice they were going to roll, what would you guess?" 
His answer "I'd definitely go with 7". "Always?". "Always." Proud parenting moment there. &lt;/p&gt;
&lt;h5&gt;Bonus&lt;/h5&gt;
&lt;p&gt;What are the true probabilities of each outcome? The 'theoretical' approach to solving the problem is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is one way to make 2: (1-1)&lt;/li&gt;
&lt;li&gt;There are two ways to make 3: (1-2) and (2-1)&lt;/li&gt;
&lt;li&gt;There are three ways to make 4: (1-3) and (2-2) and (3-1)&lt;/li&gt;
&lt;li&gt;There are four ways to make 5: (1-4) and (2-3) and (3-2) and (4-1)&lt;/li&gt;
&lt;li&gt;There are five ways to make 6: (1-5) and (2-4) and (3-3) and (4-2) and (5-1)&lt;/li&gt;
&lt;li&gt;There are six ways to make 7: (1-6) and (2-5) and (3-4) and (4-3) and (5-2) and (1-6)&lt;/li&gt;
&lt;li&gt;There are five ways to make 8: similar logic to ways to make six&lt;/li&gt;
&lt;li&gt;There are four ways to make 9: similar logic to ways to make five&lt;/li&gt;
&lt;li&gt;There are three ways to make 10: similar logic to ways to make four&lt;/li&gt;
&lt;li&gt;There are two ways to make 11: similar logic to ways to make three&lt;/li&gt;
&lt;li&gt;There is one way to make 12: similar logic to ways to make one&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are a total of 36 outcomes when you roll two dice. 6 of those outcomes will sum to 7, so 6/36 or 1/6th chance of rolling a seven. The full list of probabilities is as follows:&lt;/p&gt;
&lt;table cellpadding=10&gt;
  &lt;tr&gt;&lt;th&gt;2&lt;/th&gt;&lt;th&gt;3&lt;/th&gt;&lt;th&gt;4&lt;/th&gt;&lt;th&gt;5&lt;/th&gt;&lt;th&gt;6&lt;/th&gt;&lt;th&gt;7&lt;/th&gt;&lt;th&gt;8&lt;/th&gt;&lt;th&gt;9&lt;/th&gt;&lt;th&gt;10&lt;/th&gt;&lt;th&gt;11&lt;/th&gt;&lt;th&gt;12&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;1/36&lt;/td&gt;&lt;td&gt;1/18&lt;/td&gt;&lt;td&gt;1/12&lt;/td&gt;&lt;td&gt;1/9&lt;/td&gt;&lt;td&gt;5/36&lt;/td&gt;&lt;td&gt;1/6&lt;/td&gt;&lt;td&gt;5/36&lt;/td&gt;&lt;td&gt;1/9&lt;/td&gt;&lt;td&gt;1/12&lt;/td&gt;&lt;td&gt;1/18&lt;/td&gt;&lt;td&gt;1/36&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;</content><category term="Teaching Programming"></category><category term="dice"></category><category term="probability"></category><category term="python"></category></entry><entry><title>String.length(): I Do Not Think It Means What You Think It Means</title><link href="https://kylemcintyre.com/stringlength-i-do-not-think-it-means-what-you-think-it-means.html" rel="alternate"></link><published>2021-03-09T17:55:00-07:00</published><updated>2021-03-09T17:55:00-07:00</updated><author><name>Kyle McIntyre</name></author><id>tag:kylemcintyre.com,2021-03-09:/stringlength-i-do-not-think-it-means-what-you-think-it-means.html</id><summary type="html">&lt;p&gt;&lt;img alt="Image of Princess Bride Inconceivable Guy" src="images/inconceivable.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Today I inconceivably discovered that String.length() doesn’t mean what I thought it did. I knew it wasn’t 
the number of bytes that it took to store the string, so I assumed it was the number of logical characters in the string. 
However, depending on your definition of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Image of Princess Bride Inconceivable Guy" src="images/inconceivable.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Today I inconceivably discovered that String.length() doesn’t mean what I thought it did. I knew it wasn’t 
the number of bytes that it took to store the string, so I assumed it was the number of logical characters in the string. 
However, depending on your definition of ‘logical characters’, that might not be correct at all. &lt;/p&gt;
&lt;p&gt;How long is this string?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  “Hey good looking 😉”
   123456789012345678
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Based on the above, I’d say the string is 18 characters long. But String.length() returns 19. Why? Obviously something to do with that pesky emoji...&lt;/p&gt;
&lt;p&gt;A String in Java is an immutable array of char. String.length() is the length of that array i.e. the number of instances of 
char comprising the string. A char is a primitive, which has a defined size of two bytes. &lt;/p&gt;
&lt;p&gt;Why two bytes? I assume a big reason for this is because Java uses UTF-16 as its native byte-level representation of strings. 
UTF-16’s name refers to the 16 bits (2 bytes) in each “code unit”. UTF-16 uses either one or two code units to refer to any 
character (code point) in the Unicode standard. The 😉 emoji requires two. Because 16 bit code units are the building blocks of UTF-16 encodings, 
and Java chose UTF-16 to store strings internally, a char being two bytes wide makes good sense.&lt;/p&gt;
&lt;p&gt;So as it turns out, String.length() actually gives you the count of the number of code units required to store the string in a UTF-16 
representation, not the number of ‘logical characters’ - or code points as Unicode calls them - in your string. Let’s look at our example again:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;“Hey good looking 😉”&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logical length/number of code points: 18&lt;/li&gt;
&lt;li&gt;String.length(): 19&lt;/li&gt;
&lt;li&gt;Number of bytes to store in UTF-16: 19 * 2 = 38&lt;/li&gt;
&lt;li&gt;Number of bytes to store in UTF-8: 21 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I should point out that this problem isn’t just about Emoji - it’s about any characters outside of the BMP. So is String.length() 
defined in a dumb way? No, it’s exactly what you want in many cases given the choice of UTF-16 for an internal representation. 
It works great when doing internal stuff in a Java application and you can pretty much ignore this problem. But be careful when 
talking to third-party APIs. In these scenarios you’ll likely be dealing with byte counts or code point counts.&lt;br&gt;
I assumed String.length() would work out correctly with the latter in Google’s DLP API, but I was wrong. &lt;/p&gt;
&lt;p&gt;One way to get the logical length of a string variable foo, AKA the code point count, is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codePointCount&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;foo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="o"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also, here’s a handy function that’s not in the standard library, but should be?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="n"&gt;unicodeSubstring&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;Int&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="k"&gt;:&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;substring&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;offsetByCodePoints&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="o"&gt;),&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;offsetByCodePoints&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="Software Engineering"></category><category term="java"></category><category term="strings"></category><category term="unicode"></category><category term="dlp"></category></entry><entry><title>Clean install of JuptyerHub and JupyterLab on Ubuntu w/ Google OAuth &amp; qgrid</title><link href="https://kylemcintyre.com/clean-install-of-juptyerhub-and-jupyterlab-on-ubuntu-w-google-oauth-qgrid.html" rel="alternate"></link><published>2021-02-23T20:34:00-07:00</published><updated>2021-02-23T20:34:00-07:00</updated><author><name>Kyle McIntyre</name></author><id>tag:kylemcintyre.com,2021-02-23:/clean-install-of-juptyerhub-and-jupyterlab-on-ubuntu-w-google-oauth-qgrid.html</id><summary type="html">&lt;p&gt;I recently undertook the project of installing JupyterHub and JupyterLab on our 
development servers at Quiq. The servers are currently running Ubuntu 18.04.+ and are managed via Ansible. Users
access the servers solely via SSH and leverage port forwarding. My goal was to give each
user with an existing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently undertook the project of installing JupyterHub and JupyterLab on our 
development servers at Quiq. The servers are currently running Ubuntu 18.04.+ and are managed via Ansible. Users
access the servers solely via SSH and leverage port forwarding. My goal was to give each
user with an existing account access to their own JupyterLab workspace (via the hub), with access
to a shared directory for file/notebook exchange. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://tljh.jupyter.org/en/latest/"&gt;TLJH&lt;/a&gt; is a popular option, but I wasn't too keen on some of the more
opinionated choices it made, or perhaps I just wasn't comfortable with not understanding its inner-workings. So,
I endeavored to install the projects directly. I used &lt;a href="https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html"&gt;this guide from JupyterHub&lt;/a&gt; 
as a reference for how to install them 'from the ground up'. It was an immensely helpful resource, but it has some quirks/drawbacks that I 
tried to address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It isn't in a reproducible/repeatable form&lt;/li&gt;
&lt;li&gt;It uses a virtual env based on the system python for the JuptyerHub runtime, and a separate conda-based runtime for the user kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second point was particularly problematic. For one, basing anything off the system python gives you older versions of everything and makes it tempting 
to mess with the system python to resolve dependency hell issues - which is obviously a bad idea. The other problem
was caused by version mismatches between the jupyter hub runtime and the kernel runtime. Specifically, I struggled to get the awesome qgrid project
to work with symptoms identical to &lt;a href="https://github.com/quantopian/qgrid/issues/183"&gt;this issue&lt;/a&gt; which sites version mismatches as the likely culprit.
I eventually scrapped the approach of having two separate python runtimes and just focused on a single conda-based runtime. Having two runtimes
would've been fine too - the main problems came from basing anything off of the system python, but I collapsed down to just a single runtime for simplicity. &lt;/p&gt;
&lt;h3&gt;The Goods&lt;/h3&gt;
&lt;p&gt;The non-ansibled result of my efforts are in available in &lt;a href="https://gist.github.com/kylejmcintyre/3c0bd9a70c8693d50d23203bbadbce5d"&gt;this gist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following are notable things about this gist relative to Jupyter's 'from the ground up' guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It assumes miniconda has been installed to /opt/miniconda&lt;/li&gt;
&lt;li&gt;It creates and uses a single new conda-based python environment that is completely separate from the system python&lt;/li&gt;
&lt;li&gt;It doesn't install a nodejs-based reverse proxy server - users access JupyterHub by establishing port forwarding on port 8889&lt;/li&gt;
&lt;li&gt;It installs the OAuthenticator project and leverages Google OAuth to sign users in&lt;/li&gt;
&lt;li&gt;The Google OAuth callback assumes users forward the port locally through 8089&lt;/li&gt;
&lt;li&gt;It installs the qgrid package &amp;amp; extension&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Yes, it's a bit redundant that my users must go through the Google OAuth flow since they must be logged into the server 
in order to forward the necessary port. But JuptyerHub has no way to know that and the users breeze through it. I confirmed 
that when using Google OAuth, if the username preceding the @ in the Google account matches a user account on the server, 
that account is used for starting the kernel and their home directory is visible in the file explorer. If your users' Google
accounts don't match their usernames on your servers, I believe you can fix this via &lt;a href="https://jupyterhub.readthedocs.io/en/stable/reference/authenticators.html"&gt;username normalization&lt;/a&gt; - but I didn't have to.&lt;/p&gt;
&lt;p&gt;In order to accomplish basic file sharing, I simply created a shared directory on the servers and insured that all users had
a &lt;code&gt;shared&lt;/code&gt; symlink in their home directory pointing to that location. In the end, the install script and setup is fairly minimal
but it took me enough trial and error that I wanted to share it with the world. Cheers!&lt;/p&gt;
&lt;script src="https://gist.github.com/kylejmcintyre/3c0bd9a70c8693d50d23203bbadbce5d.js"&gt;&lt;/script&gt;</content><category term="Software Engineering"></category><category term="python"></category><category term="jupyter"></category><category term="ubuntu"></category><category term="qgrid"></category></entry><entry><title>Using Athena to convert individual JSON objects to Avro files</title><link href="https://kylemcintyre.com/using-athena-to-convert-individual-json-objects-to-avro-files.html" rel="alternate"></link><published>2021-02-12T10:20:00-07:00</published><updated>2021-02-12T10:20:00-07:00</updated><author><name>Kyle McIntyre</name></author><id>tag:kylemcintyre.com,2021-02-12:/using-athena-to-convert-individual-json-objects-to-avro-files.html</id><summary type="html">&lt;p&gt;Back in September of 2019, AWS added support to Athena to enable Create Table as Select (CTAS) statements,
as well as INSERT INTO statements. These statements allow the the results of a query to be used
to construct a new table (CTAS) or appended to another table (INSERT INTO). This …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back in September of 2019, AWS added support to Athena to enable Create Table as Select (CTAS) statements,
as well as INSERT INTO statements. These statements allow the the results of a query to be used
to construct a new table (CTAS) or appended to another table (INSERT INTO). This was a big step forward for
Athena because having this support enables it to fulfill some ETL needs. A primary use case is to convert the format
of data that underlies an Athena table.&lt;/p&gt;
&lt;p&gt;It's certainly not unusual for apps to produce individual JSON records and store them as objects in S3. And
with Athena, you can define a lazy schema that enables Presto (under the hood of Athena) to do some nice
distributed queries against them (asynchronously). However, a bunch of individual objects result in poorer
query performance as a dataset gets larger. So it's recommended to convert your data into a 'big data' file
format such as ORC, Parquet or Avro. Each of these formats have tradeoffs, but Avro is definitely the
outlier of the three under discussion. ORC and Parquet are columnar formats, whereas Avro is row-based. That
makes the structure of Avro more akin to a relational database table or spreadsheet, albeit very nicely 
compressed and with its schema declared in the file header. Roughly speaking, the columnar formats are what
you want to use if you're planning to do lots of 'analytical' queries involving a few columns, such as sums 
and averages grouped by certain columns. However, if you're planning to essentially select a bunch of full rows
back out of your big data files, Avro is arguably a better fit since reconstructing full rows out of columnar
formats can involve a lot of jumping around, especially if your data is rather wide. In a recent use case I needed
to support fetching a lot of full rows of fairly wide data, so I endeavored to get Athena to do a conversion from JSON to Avro for me. 
In the end it all worked great and I very much enjoyed the serverless and mostly "codeless" approach. However,
I did hit one snag worth sharing with the world.&lt;/p&gt;
&lt;p&gt;To set the stage, consider the following Athena table declarations, which are actually Hive DDL:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;EXTERNAL&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="k"&gt;IF&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;EXISTS&lt;/span&gt; &lt;span class="n"&gt;activity_json_staging&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;activityId&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;startTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;endTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;contactState&lt;/span&gt; &lt;span class="n"&gt;STRUCT&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;creationDate&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="k"&gt;ROW&lt;/span&gt; &lt;span class="n"&gt;FORMAT&lt;/span&gt; &lt;span class="n"&gt;serde&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;org.apache.hive.hcatalog.data.JsonSerDe&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;LOCATION&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;s3://foo-bucket/activity-json-staging&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;EXTERNAL&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="k"&gt;IF&lt;/span&gt; &lt;span class="k"&gt;NOT&lt;/span&gt; &lt;span class="k"&gt;EXISTS&lt;/span&gt; &lt;span class="n"&gt;activity_avro_base&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;activityId&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;startTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;endTime&lt;/span&gt; &lt;span class="k"&gt;TIMESTAMP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;contactState&lt;/span&gt; &lt;span class="n"&gt;STRUCT&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;STRING&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="n"&gt;PARTITIONED&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;creationDate&lt;/span&gt; &lt;span class="nb"&gt;DATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="k"&gt;ROW&lt;/span&gt; &lt;span class="n"&gt;FORMAT&lt;/span&gt; &lt;span class="n"&gt;serde&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;org.apache.hadoop.hive.serde2.avro.AvroSerDe&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;WITH&lt;/span&gt; &lt;span class="n"&gt;SERDEPROPERTIES&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avro.schema.literal&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;{&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;type&amp;quot; : &amp;quot;record&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;name&amp;quot; : &amp;quot;ContactActivityRecord&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;namespace&amp;quot; : &amp;quot;default&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;  &amp;quot;fields&amp;quot; : [ {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;activityId&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;string&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;  }, {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;startTime&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;long&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;  }, {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;endTime&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;long&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;  }, {&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;name&amp;quot; : &amp;quot;contactState&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;    &amp;quot;type&amp;quot; : {&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;type&amp;quot; : &amp;quot;record&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;name&amp;quot; : &amp;quot;Contact&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;namespace&amp;quot; : &amp;quot;default&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;      &amp;quot;fields&amp;quot; : [ {&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;quot;name&amp;quot; : &amp;quot;raw&amp;quot;,&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;quot;type&amp;quot; : [ &amp;quot;null&amp;quot;, &amp;quot;string&amp;quot; ],&lt;/span&gt;
&lt;span class="s1"&gt;        &amp;quot;default&amp;quot; : null&lt;/span&gt;
&lt;span class="s1"&gt;      } ]&lt;/span&gt;
&lt;span class="s1"&gt;    }&lt;/span&gt;
&lt;span class="s1"&gt;  } ]&lt;/span&gt;
&lt;span class="s1"&gt;}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;STORED&lt;/span&gt; &lt;span class="k"&gt;AS&lt;/span&gt; &lt;span class="n"&gt;AVRO&lt;/span&gt;
&lt;span class="k"&gt;LOCATION&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;s3://foo-bucket/activity-avro-base&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first table is a JSON-backed table, while the second is Avro-backed. This is determined by the serde declaration. With Avro, you're expected to provide
an Avro schema declaration and pass it in as a literal via SERDEPROPERTIES. This is kind of annoying since it feels like you're just repeating the Hive schema in another dialect. 
An important exception to that statement is that you don't declare avro schema for any fields that are part of the table's partition declaration. 
In the end I created a POJO-like data structure for declaring what an Athena table looks like, and generated the Hive DDL and Avro schema from that. &lt;/p&gt;
&lt;p&gt;So the general idea is to use an INSERT INTO statement to take the individual JSON records underlying the activity_json_staging table and move them 
into activity_avro_base. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;INSERT&lt;/span&gt; &lt;span class="k"&gt;INTO&lt;/span&gt; &lt;span class="n"&gt;activity_json_staging&lt;/span&gt; &lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;activity_avro_base&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As a byproduct of them being moved, their format will be changed, they'll be batched and they'll be partitioned because
the second table has a partition declared. Pretty slick, right?  However, upon testing the data transfer, I was confronted with a 
runtime error that provided essentially zero guidance and for which I couldn't find any relevant resources on the internet!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;HIVE_WRITER_DATA_ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;HIVE_WRITER_DATA_ERROR&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; &lt;span class="n"&gt;If&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="n"&gt;manifest&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="n"&gt;was&lt;/span&gt; &lt;span class="n"&gt;generated&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; 
&lt;span class="s1"&gt;&amp;#39;s3://foo-bucket/query-results/e52567d6-b56f-4fde-8514-8887d6f58f22-manifest.csv&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; 
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;may&lt;/span&gt; &lt;span class="n"&gt;need&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;manually&lt;/span&gt; &lt;span class="n"&gt;clean&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;locations&lt;/span&gt; &lt;span class="n"&gt;specified&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt; 
&lt;span class="n"&gt;Athena&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;delete&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;your&lt;/span&gt; &lt;span class="n"&gt;account&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And...that was it. I scoured the API results, CLI output and the AWS UI for more information but couldn't find any. And all Google turned up was the &lt;a href="https://github.com/prestodb/presto/blob/master/presto-hive/src/main/java/com/facebook/presto/hive/RecordFileWriter.java"&gt;Presto source code&lt;/a&gt; that presumably emits it. 
So, it was one of those memorable experiences where you basically binary search your schema/input for the problem until you pinpoint it. In the end,
the problem was that one of my JSON records had the value &lt;code&gt;null&lt;/code&gt; for contactState. The Hive DDL was fine with this, but the Avro schema was not. The fix was to 
change the Avro declaration for the contactState field as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;contactState&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;record&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Contact&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;namespace&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;fields&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;name&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;raw&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;string&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;default&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt;
      &lt;span class="p"&gt;}]&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;null&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that its type property is now an array and includes a string value of "null". Yep, that was it. Silly me! Things have been working great since then but I have lingering concerns about receiving such 
minimal diagnostic info on what seems like a pretty common mistake. Anyway, sharing it here in hopes it helps someone else. You may be wondering why I'm doing this in Athena instead of explicitly in Glue,
and I guess the answer is to limit my cloud-specific dependencies. &lt;/p&gt;</content><category term="Software Engineering"></category><category term="athena"></category><category term="avro"></category></entry></feed>